# Generated file: !!! DO NOT EDIT !!!
---
env:
  PYPERFORMANCE_HASH: 16765924a3e845fe9c3c582b7576e5f1285955df
  PYSTON_BENCHMARKS_HASH: d9506cae863bb5c199ea71dce6a1733a12ba3ab8
name: benchmark
on:
  workflow_dispatch:
    inputs:
      fork:
        description: Fork of cpython to benchmark
        default: python
      ref:
        description: Branch, tag or (full) SHA commit to benchmark
        default: main
      benchmarks:
        description: Benchmarks to run (comma-separated; empty runs all benchmarks)
        type: string
      benchmark_base:
        description: Also benchmark the base of this commit
        type: boolean
        default: false
      pystats:
        description: Also collect pystats for this commit
        type: boolean
        default: false
      pystats_individual:
        description: Collect pystats for each individual benchmark
        type: boolean
        default: false
      tier2:
        description: Enable the Tier 2 interpreter
        type: boolean
        default: false
      jit:
        description: Enable the JIT
        type: boolean
        default: false

jobs:
  # Determine the base commit of the selected commit. The output is passed to
  # the `base` job below. If the data already exists for this commit, it will be
  # skipped.
  determine_base:
    runs-on: ubuntu-latest
    outputs:
      ref: ${{ steps.base.outputs.ref }}
      need_to_run: ${{ steps.base.outputs.need_to_run }}
    steps:
    - name: Checkout benchmarking
      uses: actions/checkout@v4
    - name: Setup system Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: pip
    - name: Checkout CPython
      uses: actions/checkout@v4
      if: ${{ inputs.benchmark_base }}
      with:
        repository: ${{ inputs.fork }}/cpython
        path: cpython
        ref: ${{ inputs.ref }}
        fetch-depth: 0
    - name: Install dependencies from PyPI
      run: |
        pip install -r requirements.txt
    - name: Determine base
      id: base
      run: |
        python -m bench_runner get_merge_base ${{ inputs.benchmark_base }} all ${{ inputs.pystats }} ${{ inputs.tier2 }} ${{ inputs.jit }} >> $GITHUB_OUTPUT
        cat $GITHUB_OUTPUT

  head:
    uses: ./.github/workflows/_benchmark.yml
    with:
      fork: ${{ inputs.fork }}
      ref: ${{ inputs.ref }}
      benchmarks: ${{ inputs.benchmarks }}
      pgo: true
      perf: false
      force: true
      tier2: ${{ inputs.tier2 }}
      jit: ${{ inputs.jit }}
    secrets: inherit

  base:
    uses: ./.github/workflows/_benchmark.yml
    needs: determine_base
    if: ${{ needs.determine_base.outputs.need_to_run != 'false' }}
    with:
      fork: python
      ref: ${{ needs.determine_base.outputs.ref }}
      benchmarks: ${{ inputs.benchmarks }}
      pgo: true
      perf: false
      force: false
      tier2: ${{ inputs.tier2 }}
      jit: ${{ inputs.jit }}
    secrets: inherit

  pystats-head:
    uses: ./.github/workflows/_pystats.yml
    if: ${{ inputs.pystats }}
    with:
      fork: ${{ inputs.fork }}
      ref: ${{ inputs.ref }}
      benchmarks: ${{ inputs.benchmarks }}
      individual: ${{ inputs.pystats_individual }}
      force: true
      tier2: ${{ inputs.tier2 }}
    secrets: inherit

  pystats-base:
    uses: ./.github/workflows/_pystats.yml
    needs: determine_base
    if: ${{ inputs.pystats && needs.determine_base.outputs.need_to_run != 'false'
      }}
    with:
      fork: python
      ref: ${{ needs.determine_base.outputs.ref }}
      benchmarks: ${{ inputs.benchmarks }}
      individual: ${{ inputs.pystats_individual }}
      force: false
      tier2: ${{ inputs.tier2 }}
    secrets: inherit

  generate:
    uses: ./.github/workflows/_generate.yml
    with:
      force: false
    if: ${{ always() }}
    needs: [head, base, pystats-head, pystats-base]
    secrets: inherit

  publish:
    uses: ./.github/workflows/_publish.yml
    if: ${{ always() }}
    needs: [generate]
    secrets: inherit
